{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dy.parameter(...) call is now DEPRECATED.\n",
      "        There is no longer need to explicitly add parameters to the computation graph.\n",
      "        Any used parameter will be added automatically.\n",
      "--finished 1000 sentences\n",
      "--finished 2000 sentences\n",
      "--finished 3000 sentences\n",
      "--finished 4000 sentences\n",
      "--finished 5000 sentences\n",
      "--finished 6000 sentences\n",
      "--finished 7000 sentences\n",
      "--finished 8000 sentences\n",
      "--finished 9000 sentences\n",
      "--finished 10000 sentences\n",
      "iter 0: train loss/word=26.7125, kl loss/word=0.0039, reconstruction loss/word=26.7087, ppl=399118142295.9409, time=609.81s\n",
      "iter 0: dev loss/word=25.0204, kl loss/word=0.0003, reconstruction loss/word=25.0201, ppl=73491332018.4293, time=14.37s\n",
      "--finished 1000 sentences\n",
      "--finished 2000 sentences\n",
      "--finished 3000 sentences\n",
      "--finished 4000 sentences\n",
      "--finished 5000 sentences\n",
      "--finished 6000 sentences\n",
      "--finished 7000 sentences\n",
      "--finished 8000 sentences\n",
      "--finished 9000 sentences\n",
      "--finished 10000 sentences\n",
      "iter 1: train loss/word=23.6522, kl loss/word=0.0004, reconstruction loss/word=23.6518, ppl=18707562278.8708, time=731.38s\n",
      "iter 1: dev loss/word=24.7117, kl loss/word=0.0003, reconstruction loss/word=24.7114, ppl=53971262786.0396, time=17.85s\n",
      "--finished 1000 sentences\n",
      "--finished 2000 sentences\n",
      "--finished 3000 sentences\n",
      "--finished 4000 sentences\n",
      "--finished 5000 sentences\n",
      "--finished 6000 sentences\n",
      "--finished 7000 sentences\n",
      "--finished 8000 sentences\n",
      "--finished 9000 sentences\n",
      "--finished 10000 sentences\n",
      "iter 2: train loss/word=22.3251, kl loss/word=0.0004, reconstruction loss/word=22.3247, ppl=4962119531.2013, time=787.56s\n",
      "iter 2: dev loss/word=24.7495, kl loss/word=0.0004, reconstruction loss/word=24.7491, ppl=56050143776.3442, time=18.82s\n",
      "--finished 1000 sentences\n",
      "--finished 2000 sentences\n",
      "--finished 3000 sentences\n",
      "--finished 4000 sentences\n",
      "--finished 5000 sentences\n",
      "--finished 6000 sentences\n",
      "--finished 7000 sentences\n",
      "--finished 8000 sentences\n",
      "--finished 9000 sentences\n",
      "--finished 10000 sentences\n",
      "iter 3: train loss/word=21.3052, kl loss/word=0.0004, reconstruction loss/word=21.3048, ppl=1789564083.8901, time=799.14s\n",
      "iter 3: dev loss/word=24.8936, kl loss/word=0.0008, reconstruction loss/word=24.8928, ppl=64733994377.6660, time=17.43s\n",
      "--finished 1000 sentences\n",
      "--finished 2000 sentences\n",
      "--finished 3000 sentences\n",
      "--finished 4000 sentences\n",
      "--finished 5000 sentences\n",
      "--finished 6000 sentences\n",
      "--finished 7000 sentences\n",
      "--finished 8000 sentences\n",
      "--finished 9000 sentences\n",
      "--finished 10000 sentences\n",
      "iter 4: train loss/word=20.4117, kl loss/word=0.0004, reconstruction loss/word=20.4113, ppl=732304601.6403, time=804.01s\n",
      "iter 4: dev loss/word=25.0794, kl loss/word=0.0004, reconstruction loss/word=25.0791, ppl=77957416822.1565, time=17.06s\n",
      "--finished 1000 sentences\n",
      "--finished 2000 sentences\n",
      "--finished 3000 sentences\n",
      "--finished 4000 sentences\n",
      "--finished 5000 sentences\n",
      "--finished 6000 sentences\n",
      "--finished 7000 sentences\n",
      "--finished 8000 sentences\n",
      "--finished 9000 sentences\n",
      "--finished 10000 sentences\n",
      "iter 5: train loss/word=19.6243, kl loss/word=0.0004, reconstruction loss/word=19.6240, ppl=333229321.2475, time=945.15s\n",
      "iter 5: dev loss/word=25.3738, kl loss/word=0.0001, reconstruction loss/word=25.3737, ppl=104644045113.0260, time=30.31s\n",
      "--finished 1000 sentences\n",
      "--finished 2000 sentences\n",
      "--finished 3000 sentences\n",
      "--finished 4000 sentences\n",
      "--finished 5000 sentences\n",
      "--finished 6000 sentences\n",
      "--finished 7000 sentences\n",
      "--finished 8000 sentences\n",
      "--finished 9000 sentences\n",
      "--finished 10000 sentences\n",
      "iter 6: train loss/word=18.9698, kl loss/word=0.0001, reconstruction loss/word=18.9697, ppl=173168753.8257, time=865.18s\n",
      "iter 6: dev loss/word=25.6549, kl loss/word=0.0001, reconstruction loss/word=25.6547, ppl=138600838413.4064, time=19.58s\n",
      "--finished 1000 sentences\n",
      "--finished 2000 sentences\n",
      "--finished 3000 sentences\n",
      "--finished 4000 sentences\n",
      "--finished 5000 sentences\n",
      "--finished 6000 sentences\n",
      "--finished 7000 sentences\n",
      "--finished 8000 sentences\n",
      "--finished 9000 sentences\n",
      "--finished 10000 sentences\n",
      "iter 7: train loss/word=18.3609, kl loss/word=0.0001, reconstruction loss/word=18.3608, ppl=94194826.5674, time=861.77s\n",
      "iter 7: dev loss/word=26.0632, kl loss/word=0.0001, reconstruction loss/word=26.0631, ppl=208503532062.0839, time=19.84s\n",
      "--finished 1000 sentences\n",
      "--finished 2000 sentences\n",
      "--finished 3000 sentences\n",
      "--finished 4000 sentences\n",
      "--finished 5000 sentences\n",
      "--finished 6000 sentences\n",
      "--finished 7000 sentences\n",
      "--finished 8000 sentences\n",
      "--finished 9000 sentences\n",
      "--finished 10000 sentences\n",
      "iter 8: train loss/word=17.8011, kl loss/word=0.0001, reconstruction loss/word=17.8010, ppl=53818001.7016, time=874.65s\n",
      "iter 8: dev loss/word=26.3825, kl loss/word=0.0001, reconstruction loss/word=26.3824, ppl=286921893983.4008, time=19.31s\n",
      "--finished 1000 sentences\n",
      "--finished 2000 sentences\n",
      "--finished 3000 sentences\n",
      "--finished 4000 sentences\n",
      "--finished 5000 sentences\n",
      "--finished 6000 sentences\n",
      "--finished 7000 sentences\n",
      "--finished 8000 sentences\n",
      "--finished 9000 sentences\n",
      "--finished 10000 sentences\n",
      "iter 9: train loss/word=17.3215, kl loss/word=0.0001, reconstruction loss/word=17.3214, ppl=33314320.5156, time=886.93s\n",
      "iter 9: dev loss/word=26.8329, kl loss/word=0.0000, reconstruction loss/word=26.8329, ppl=450174502244.7604, time=19.63s\n",
      "--finished 1000 sentences\n",
      "--finished 2000 sentences\n",
      "--finished 3000 sentences\n",
      "--finished 4000 sentences\n",
      "--finished 5000 sentences\n",
      "--finished 6000 sentences\n",
      "--finished 7000 sentences\n",
      "--finished 8000 sentences\n",
      "--finished 9000 sentences\n",
      "--finished 10000 sentences\n",
      "iter 10: train loss/word=16.8784, kl loss/word=0.0001, reconstruction loss/word=16.8783, ppl=21389297.1857, time=1044.04s\n",
      "iter 10: dev loss/word=27.1355, kl loss/word=0.0000, reconstruction loss/word=27.1355, ppl=609258513546.6230, time=31.12s\n",
      "--finished 1000 sentences\n",
      "--finished 2000 sentences\n",
      "--finished 3000 sentences\n",
      "--finished 4000 sentences\n",
      "--finished 5000 sentences\n",
      "--finished 6000 sentences\n",
      "--finished 7000 sentences\n",
      "--finished 8000 sentences\n",
      "--finished 9000 sentences\n",
      "--finished 10000 sentences\n",
      "iter 11: train loss/word=16.4868, kl loss/word=0.0000, reconstruction loss/word=16.4868, ppl=14459130.1599, time=1442.44s\n",
      "iter 11: dev loss/word=27.3534, kl loss/word=0.0001, reconstruction loss/word=27.3533, ppl=757556479257.2745, time=30.31s\n",
      "--finished 1000 sentences\n",
      "--finished 2000 sentences\n",
      "--finished 3000 sentences\n",
      "--finished 4000 sentences\n",
      "--finished 5000 sentences\n",
      "--finished 6000 sentences\n",
      "--finished 7000 sentences\n",
      "--finished 8000 sentences\n",
      "--finished 9000 sentences\n",
      "--finished 10000 sentences\n",
      "iter 12: train loss/word=16.1162, kl loss/word=0.0000, reconstruction loss/word=16.1162, ppl=9981525.2332, time=1020.31s\n",
      "iter 12: dev loss/word=27.5559, kl loss/word=0.0001, reconstruction loss/word=27.5558, ppl=927665730317.0084, time=17.40s\n",
      "--finished 1000 sentences\n",
      "--finished 2000 sentences\n",
      "--finished 3000 sentences\n",
      "--finished 4000 sentences\n",
      "--finished 5000 sentences\n",
      "--finished 6000 sentences\n",
      "--finished 7000 sentences\n",
      "--finished 8000 sentences\n",
      "--finished 9000 sentences\n",
      "--finished 10000 sentences\n",
      "iter 13: train loss/word=15.7900, kl loss/word=0.0001, reconstruction loss/word=15.7900, ppl=7203063.0395, time=669.88s\n",
      "iter 13: dev loss/word=27.9054, kl loss/word=0.0001, reconstruction loss/word=27.9053, ppl=1315718174172.1484, time=13.81s\n",
      "--finished 1000 sentences\n",
      "--finished 2000 sentences\n",
      "--finished 3000 sentences\n",
      "--finished 4000 sentences\n",
      "--finished 5000 sentences\n",
      "--finished 6000 sentences\n",
      "--finished 7000 sentences\n",
      "--finished 8000 sentences\n",
      "--finished 9000 sentences\n",
      "--finished 10000 sentences\n",
      "iter 14: train loss/word=16.2230, kl loss/word=0.3726, reconstruction loss/word=15.8504, ppl=11105802.1026, time=610.61s\n",
      "iter 14: dev loss/word=29.3095, kl loss/word=0.9348, reconstruction loss/word=28.3747, ppl=5357640235738.5557, time=13.22s\n",
      "--finished 1000 sentences\n",
      "--finished 2000 sentences\n",
      "--finished 3000 sentences\n",
      "--finished 4000 sentences\n",
      "--finished 5000 sentences\n",
      "--finished 6000 sentences\n",
      "--finished 7000 sentences\n",
      "--finished 8000 sentences\n",
      "--finished 9000 sentences\n",
      "--finished 10000 sentences\n",
      "iter 15: train loss/word=17.6502, kl loss/word=1.6655, reconstruction loss/word=15.9847, ppl=46280873.2192, time=592.56s\n",
      "iter 15: dev loss/word=29.9449, kl loss/word=1.6754, reconstruction loss/word=28.2695, ppl=10113431340865.3301, time=13.17s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--finished 1000 sentences\n",
      "--finished 2000 sentences\n",
      "--finished 3000 sentences\n",
      "--finished 4000 sentences\n",
      "--finished 5000 sentences\n",
      "--finished 6000 sentences\n",
      "--finished 7000 sentences\n",
      "--finished 8000 sentences\n",
      "--finished 9000 sentences\n",
      "--finished 10000 sentences\n",
      "iter 16: train loss/word=21.5822, kl loss/word=5.6924, reconstruction loss/word=15.8898, ppl=2360574464.1200, time=614.87s\n",
      "iter 16: dev loss/word=35.2552, kl loss/word=6.8942, reconstruction loss/word=28.3610, ppl=2047048591177590.7500, time=16.73s\n",
      "--finished 1000 sentences\n",
      "--finished 2000 sentences\n",
      "--finished 3000 sentences\n",
      "--finished 4000 sentences\n",
      "--finished 5000 sentences\n",
      "--finished 6000 sentences\n",
      "--finished 7000 sentences\n",
      "--finished 8000 sentences\n",
      "--finished 9000 sentences\n",
      "--finished 10000 sentences\n",
      "iter 17: train loss/word=29.9140, kl loss/word=13.6737, reconstruction loss/word=16.2403, ppl=9806330501712.2812, time=656.80s\n",
      "iter 17: dev loss/word=43.7025, kl loss/word=14.9950, reconstruction loss/word=28.7075, ppl=9544325055588007936.0000, time=14.37s\n",
      "--finished 1000 sentences\n",
      "--finished 2000 sentences\n",
      "--finished 3000 sentences\n",
      "--finished 4000 sentences\n",
      "--finished 5000 sentences\n",
      "--finished 6000 sentences\n",
      "--finished 7000 sentences\n",
      "--finished 8000 sentences\n",
      "--finished 9000 sentences\n",
      "--finished 10000 sentences\n",
      "iter 18: train loss/word=34.7119, kl loss/word=16.8949, reconstruction loss/word=17.8170, ppl=1188975417183008.5000, time=630.94s\n",
      "iter 18: dev loss/word=43.7120, kl loss/word=12.5974, reconstruction loss/word=31.1146, ppl=9636078578369839104.0000, time=14.29s\n",
      "--finished 1000 sentences\n",
      "--finished 2000 sentences\n",
      "--finished 3000 sentences\n",
      "--finished 4000 sentences\n",
      "--finished 5000 sentences\n",
      "--finished 6000 sentences\n",
      "--finished 7000 sentences\n",
      "--finished 8000 sentences\n",
      "--finished 9000 sentences\n",
      "--finished 10000 sentences\n",
      "iter 19: train loss/word=72.5999, kl loss/word=48.6020, reconstruction loss/word=23.9979, ppl=33864720589924041634350180073472.0000, time=688.69s\n",
      "iter 19: dev loss/word=136.2897, kl loss/word=104.4928, reconstruction loss/word=31.7969, ppl=154830490024690865921404578978090783390796202939596467601408.0000, time=14.33s\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Magnitude of gradient is bad: inf",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-c01b933f0faa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mtrain_words\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[0mtotal_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msent_id\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--finished %r sentences\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msent_id\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_dynet.pyx\u001b[0m in \u001b[0;36m_dynet.Trainer.update\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_dynet.pyx\u001b[0m in \u001b[0;36m_dynet.Trainer.update\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Magnitude of gradient is bad: inf"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "import dynet as dy\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "# much of the beginning is the same as the text retrieval\n",
    "# format of files: each line is \"word1 word2 ...\" aligned line-by-line\n",
    "train_src_file = \"../data/parallel/train.ja\"\n",
    "train_trg_file = \"../data/parallel/train.en\"\n",
    "dev_src_file = \"../data/parallel/dev.ja\"\n",
    "dev_trg_file = \"../data/parallel/dev.en\"\n",
    "test_src_file = \"../data/parallel/test.ja\"\n",
    "test_trg_file = \"../data/parallel/test.en\"\n",
    "\n",
    "w2i_src = defaultdict(lambda: len(w2i_src))\n",
    "w2i_trg = defaultdict(lambda: len(w2i_trg))\n",
    "\n",
    "\n",
    "def read(fname_src, fname_trg):\n",
    "    #return text.decode(result['encoding'])\n",
    "    \"\"\"\n",
    "    Read parallel files where each line lines up\n",
    "    \"\"\"\n",
    "    with open(fname_src, \"r\",encoding='utf-8') as f_src, open(fname_trg, \"r\",encoding='utf-8') as f_trg:\n",
    "        #file = open(filename, encoding=\"utf8\")\n",
    "        for line_src, line_trg in zip(f_src, f_trg):\n",
    "            # need to append EOS tags to at least the target sentence\n",
    "            sent_src = [w2i_src[x] for x in line_src.strip().split() + ['</s>']]\n",
    "            sent_trg = [w2i_trg[x] for x in ['<s>'] + line_trg.strip().split() + ['</s>']]\n",
    "            yield (sent_src, sent_trg)\n",
    "\n",
    "\n",
    "# Read the data\n",
    "train = list(read(train_src_file, train_trg_file))\n",
    "unk_src = w2i_src[\"<unk>\"]\n",
    "eos_src = w2i_src['</s>']\n",
    "w2i_src = defaultdict(lambda: unk_src, w2i_src)\n",
    "unk_trg = w2i_trg[\"<unk>\"]\n",
    "eos_trg = w2i_trg['</s>']\n",
    "sos_trg = w2i_trg['<s>']\n",
    "w2i_trg = defaultdict(lambda: unk_trg, w2i_trg)\n",
    "i2w_trg = {v: k for k, v in w2i_trg.items()}\n",
    "\n",
    "nwords_src = len(w2i_src)\n",
    "nwords_trg = len(w2i_trg)\n",
    "dev = list(read(dev_src_file, dev_trg_file))\n",
    "test = list(read(test_src_file, test_trg_file))\n",
    "# DyNet Starts\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)\n",
    "\n",
    "# Model parameters\n",
    "EMBED_SIZE = 64\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Especially in early training, the model can generate basically infinitly without generating an EOS\n",
    "# have a max sent size that you end at\n",
    "MAX_SENT_SIZE = 50\n",
    "\n",
    "# Lookup parameters for word embeddings\n",
    "LOOKUP_SRC = model.add_lookup_parameters((nwords_src, EMBED_SIZE))\n",
    "LOOKUP_TRG = model.add_lookup_parameters((nwords_trg, EMBED_SIZE))\n",
    "\n",
    "# Word-level LSTMs\n",
    "LSTM_SRC_BUILDER = dy.LSTMBuilder(1, EMBED_SIZE, HIDDEN_SIZE, model)\n",
    "LSTM_TRG_BUILDER = dy.LSTMBuilder(1, EMBED_SIZE, HIDDEN_SIZE, model)\n",
    "\n",
    "# The MLP parameters to compute mean variance from source output. We use the same hidden size for simplicity.\n",
    "Q_HIDDEN_SIZE = 64\n",
    "W_mean_p = model.add_parameters((Q_HIDDEN_SIZE, HIDDEN_SIZE))\n",
    "V_mean_p = model.add_parameters((HIDDEN_SIZE, Q_HIDDEN_SIZE))\n",
    "b_mean_p = model.add_parameters((Q_HIDDEN_SIZE))\n",
    "\n",
    "W_var_p = model.add_parameters((Q_HIDDEN_SIZE, HIDDEN_SIZE))\n",
    "V_var_p = model.add_parameters((HIDDEN_SIZE, Q_HIDDEN_SIZE))\n",
    "b_var_p = model.add_parameters((Q_HIDDEN_SIZE))\n",
    "\n",
    "# the softmax from the hidden size\n",
    "W_sm_p = model.add_parameters((nwords_trg, HIDDEN_SIZE))  # Weights of the softmax\n",
    "b_sm_p = model.add_parameters((nwords_trg))  # Softmax bias\n",
    "\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "    # Get z by reparameterization.\n",
    "    d = mu.dim()[0][0]\n",
    "    eps = dy.random_normal(d)\n",
    "    std = dy.exp(logvar * 0.5)\n",
    "\n",
    "    return mu + dy.cmult(std, eps)\n",
    "\n",
    "\n",
    "def mlp(x, W, V, b):\n",
    "    # A mlp with only one hidden layer.\n",
    "    return V * dy.tanh(W * x + b)\n",
    "\n",
    "\n",
    "def calc_loss(sent):\n",
    "    dy.renew_cg()\n",
    "\n",
    "    # Transduce all batch elements with an LSTM\n",
    "    src = sent[0]\n",
    "    trg = sent[1]\n",
    "\n",
    "    # initialize the LSTM\n",
    "    init_state_src = LSTM_SRC_BUILDER.initial_state()\n",
    "\n",
    "    # get the output of the first LSTM\n",
    "    src_output = init_state_src.add_inputs([LOOKUP_SRC[x] for x in src])[-1].output()\n",
    "\n",
    "    # Now compute mean and standard deviation of source hidden state.\n",
    "    W_mean = dy.parameter(W_mean_p)\n",
    "    V_mean = dy.parameter(V_mean_p)\n",
    "    b_mean = dy.parameter(b_mean_p)\n",
    "\n",
    "    W_var = dy.parameter(W_var_p)\n",
    "    V_var = dy.parameter(V_var_p)\n",
    "    b_var = dy.parameter(b_var_p)\n",
    "\n",
    "    # The mean vector from the encoder.\n",
    "    mu = mlp(src_output, W_mean, V_mean, b_mean)\n",
    "    # This is the diagonal vector of the log co-variance matrix from the encoder\n",
    "    # (regard this as log variance is easier for furture implementation)\n",
    "    log_var = mlp(src_output, W_var, V_var, b_var)\n",
    "\n",
    "    # Compute KL[N(u(x), sigma(x)) || N(0, I)]\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    kl_loss = -0.5 * dy.sum_elems(1 + log_var - dy.pow(mu, dy.inputVector([2])) - dy.exp(log_var))\n",
    "\n",
    "    z = reparameterize(mu, log_var)\n",
    "\n",
    "    # now step through the output sentence\n",
    "    all_losses = []\n",
    "\n",
    "    current_state = LSTM_TRG_BUILDER.initial_state().set_s([z, dy.tanh(z)])\n",
    "    prev_word = trg[0]\n",
    "    W_sm = dy.parameter(W_sm_p)\n",
    "    b_sm = dy.parameter(b_sm_p)\n",
    "\n",
    "    for next_word in trg[1:]:\n",
    "        # feed the current state into the\n",
    "        current_state = current_state.add_input(LOOKUP_TRG[prev_word])\n",
    "        output_embedding = current_state.output()\n",
    "\n",
    "        s = dy.affine_transform([b_sm, W_sm, output_embedding])\n",
    "        all_losses.append(dy.pickneglogsoftmax(s, next_word))\n",
    "\n",
    "        prev_word = next_word\n",
    "\n",
    "    softmax_loss = dy.esum(all_losses)\n",
    "\n",
    "    return kl_loss, softmax_loss\n",
    "\n",
    "\n",
    "for ITER in range(100):\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    train_words, train_loss, train_kl_loss, train_reconstruct_loss = 0, 0.0, 0.0, 0.0\n",
    "    start = time.time()\n",
    "    for sent_id, sent in enumerate(train):\n",
    "        kl_loss, softmax_loss = calc_loss(sent)\n",
    "        total_loss = dy.esum([kl_loss, softmax_loss])\n",
    "        train_loss += total_loss.value()\n",
    "\n",
    "        # Record the KL loss and reconstruction loss separately help you monitor the training.\n",
    "        train_kl_loss += kl_loss.value()\n",
    "        train_reconstruct_loss += softmax_loss.value()\n",
    "\n",
    "        train_words += len(sent)\n",
    "        total_loss.backward()\n",
    "        trainer.update()\n",
    "        if (sent_id + 1) % 1000 == 0:\n",
    "            print(\"--finished %r sentences\" % (sent_id + 1))\n",
    "\n",
    "    print(\"iter %r: train loss/word=%.4f, kl loss/word=%.4f, reconstruction loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (\n",
    "        ITER, train_loss / train_words, train_kl_loss / train_words, train_reconstruct_loss / train_words,\n",
    "        math.exp(train_loss / train_words), time.time() - start))\n",
    "\n",
    "    # Evaluate on dev set\n",
    "    dev_words, dev_loss, dev_kl_loss, dev_reconstruct_loss = 0, 0.0, 0.0, 0.0\n",
    "    start = time.time()\n",
    "    for sent_id, sent in enumerate(dev):\n",
    "        kl_loss, softmax_loss = calc_loss(sent)\n",
    "\n",
    "        dev_kl_loss += kl_loss.value()\n",
    "        dev_reconstruct_loss += softmax_loss.value()\n",
    "        dev_loss += kl_loss.value() + softmax_loss.value()\n",
    "\n",
    "        dev_words += len(sent)\n",
    "        trainer.update()\n",
    "\n",
    "    print(\"iter %r: dev loss/word=%.4f, kl loss/word=%.4f, reconstruction loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (\n",
    "        ITER, dev_loss / dev_words, dev_kl_loss / dev_words, dev_reconstruct_loss / dev_words,\n",
    "        math.exp(dev_loss / dev_words), time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
